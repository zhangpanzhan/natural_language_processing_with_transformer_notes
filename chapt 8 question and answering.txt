chapt 8 question and answering

extractive QA: span of text in doc
two stage: retrieve relevant docs + extract answers from them.

dataset: SQuAD (pioneered at SubjQA)
fullname: Stanford Question Answering Dataset
format(question, review, [answer sentences])

sample english articles from wikipedia
- 1.0 version, each question has answer
- 2.0 version, not guaranteed, performed well, but not reliable, consider NQ(Natural Question) dataset

Extracting anwers from text

use pretrained model MiniLM

from transformers import AutoTokenizer
model_ckpt = "deepset/minilm-uncased-squad2"
tokenizer = AutoTokenizer.from_pretrained(model_cpkt)
tokenizer(question, context, return_tensors='pt')

from transformers import AutoModelForQuestionAnswering
model = AutoModelForQuestionAnsering.from_pretrained(model_cpkt)

from transformers import pipeline
pipe = pipeline("qeustion-ansering", model = model, tokenizer = tokenizer)
pipe(question = question, context = context, topk=3)
pipe(question = "", context = context, handle_impossible_answer=True)

Dealing with long passages

context length > maximum seq length of model

tokenizer(question, context, return_tensors='pt') =>
tokenizer(question, context, return_tensors='pt', return_overflowing_tokens=True, max_length=100, stride=25)

Using Haystack to Build a QA Pipeline

concat all relevant docs as context will make avg lantency much longer.

two stages: 

retriever:  BM25 (sparse) + DPR(dense)
- sparse: use freq as vectors and inner product as similarity
- dense: transformers to encode query and docs as embedding
reader: 
- extract answers from relevant docs

haystack library by deepset

two another components:
- docuement store: doc-oriented database storing all docs and metadata
- pipeline: all components of QA

docuement store compatible with retriever:
 - sparse: tf-idf, bm25
 - dense: embedding, dpr

choose ElasticsearchDocumentStore (compatible both)

document_store = ElasticsearchDocumentStore(return_embedding=True)

- elasticsearch server running
- instantiate elastic search document store
  - create document index storing doc
  - create label index storing answer spans

document_store.write_documents(docs, index="document")

initializing a retriever
- sparse retriever based on BM25

es_retriever = ElasticsearchRetriever(document_store=document_store)

item_id = "B0074BW614"
query = "Is it good for reading?"
retrieved_docs = es_retriever.retrieve(query=query, top_k=3, filters={"item_id":[item_id], "split":["train"]})

initializing a reader
- to load MiniLM model in haystack
- 2 types of reader
  - FARMReader: for fine-tuning and deploying; use this one.(fine-tune reading comprehension model in transformers and then reload in FARMReader)
  - TransformersReader: inference only

from haystack.reader.farm import FARMReader
model_ckpt = "deepset/minilm-uncased-squad2"
max_seq_length, doc_stride = 384, 128
reader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,
max_seq_len=max_seq_length, doc_stride=doc_stride, return_no_answer=True)

reader.predict_on_texts(question=question, texts=[context], top_k=1)

Putting it all together

from haystack.pipeline import ExtractiveQAPipeline
pipe = ExtractiveQAPipeline(reader, es_retriever)
n_answers = 3
preds = pipe.run(query=query, top_k_retriever=3, top_k_reader=n_answers, filters={"item_id": [item_id], "split":["train"]})
print(f"Question: {preds['query']} \n")
for idx in range(n_answers):
  print(f"Answer {idx+1}: {preds['answers'][idx]['answer']}")
  print(f"Review snippet: ...{preds['answers'][idx]['context']}...")
  print("\n\n")

Improving Our QA Pipleline

retriever sets upper bound on performance of QA system.

evaluate retrieval with sparse+dense representation
recall metric: recall@k

class PipelineNode
class EvalRetrieverPipeline
pipe = EvalRetrieverPipeline()
labels = ...
document_store.write_labels(labels, index="label") # write to elasticsearch
labels_agg = document_store.get_all_labels_aggregated()... # one query, multiple labels
run_pipeline(pipe, top_k_retriever=3)
pipe.eval_retriever.recall


- sparse encoders
- dense: reduce k (k relevant documents to extract answer later) to improve latency 

evaluate reader

exact match(em): common chars
f1-score

Domain Adaptation(boost performance)

SubjQA: bad
SQuAD: good

fine tune MiniLM on SubjQA

fine tune a pretrained model directly on SubjQA is bad.


Evaluating the whole QA pipeline

Eval retriever + reader

Going Beyond Extractive QA

abstractive/generative QA

- generate better-phrased answers with pretrained language model.
- synthesize evidence across multiple passages

RAG (retrieval-augmented generation)

DPR (retriever)+generator (reader)

generator: seq2seq transformer that generates answers based query and these documents.

- RAG sequence
- RAG Token: perform better