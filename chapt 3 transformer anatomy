chapt 3 transformer anatomy

intro

- explore blocks of transformer; implementation by torch and tensorflow
- other pieces to make transformer encoder to work
- check diff between encoder and decoder
- explore zoo of models in recent years

the transformer architecture

- encoder
convert input seq of tokens into a seq of embedding vectors/hidden state/context
key and value
- decoder
use encoder's hidden state to iteratively generate an output of seq of tokens, one token at a time.


transformer architecture: originally for machine translation, but later as standalone models

text tokenized -> token embeddings
text tokenized -> position embeddings

encoder or decoder is composed of layers of block
encoder's output is fed into decoder's layer;
decoder stops until it predict EOS token or reached maximum length.

3 types:

encoder-only:
- convert input seq into numerical representation
- suitable for text classification/ NER
- Bert, RoBert, DistilBert
- Bidirectional attention.

decoder-only:
- given prompt of text like "thanks for lunch, ...", autocomplete sentence by predicting next probable word.
- family of GPT
- depends only on left context
- also called causal/autoregressive attention

encoder-decoder:
- transform one seq to another seq
- for machine translation and summarization tasks
- transformer, BART, T5 models

the encoder

each encoder has input of seq of embeddings and feed them into sublayers:
1. multi-head self-attention layer
2. fully connected feed-forward layer
3. output of emebedding has same size of inputs, but update the input embeddings to encode context information in sequence; such as apple embedding update to be "company-like" and "fruit-less"
4. other components: skip connection + layer normalization

self-attention

attention: assign weight/attention to each element in a sequence.

"self": these weights are computed for all hidden states in the same set (e.g. all hidden states in encoder); query set are the same as key set

instead of using fixed embedding for each token x1,..., xn, generate new sequence of embeddings x1', xn'
xi' = linear combination of all xj = sum_j (wji . xj)
embrace context info into embedding called contextualized embeddings
"fruit flies like a banana"=> "insect"
"time flies like an arrow" => "soars"

scaled dot-product attention

- project each token embedding into 3 vectors: query, key, value
- compute attention/similarity scores
- compute attention weights:  multiplied by a scaling factor to normalize variance and then normalized with softmax to ensure column sum to 1.
- update token embeddings:  updated representation for embedding xi' = sum_j (wji.v_j)
- equal query and key vectors will assign large score to identical words in context, but "flies" is better defined by "time" and "arrow" rather than another mention of "flies".
- create different set of vectors for query, key, and value with different linear projections.

mulit-head attention

- use different linear transformation to each embedding to generate query, key and value
- allow self-attention to focus on different semantic apspects of the sequence

- multiple sets of linear projection, each one is attention head.
- why more than one head? softmax of one head tends focus on one aspect of similarity like "subject-verb" interaction, but we need more like "nearby-adjectives" interaction
- initi embedding size is 768, 12 heads, each head dim is 768/12 = 64


Adding Layer normalization

layer normalization: normalize each input in the batch to have 0 mean and 1 variance
skip connection: pass a tensor to next layer without processed and add it to the processed tensor.

2 main choice for layer normalization

post layer normalization: between skip connection

- multi-head attention with skip -> layer norm -> ffnn with skip -> layer norm
- need learning rate warm as gradients can diverge.

pre layer normalization: within skip connection

- (layer norm + multi head attention) with skip -> (layer norm + ffnn) with skip
- no need learning rate warm up

Positional encodings

method 1: use learnable pattern, when pretrained data is sufficiently large.

if a sentence has 5 words, then [0, 1, 2, 3, 4] -> nn.embeding(hidden_size)
note the diff with token embedding, [7124, 254, 13, 975, 222]-> nn.embedding(hidden_size)
the token embedding's input is seq with maximum number of tokens
the positional embedding's input is seq with maximum length of sequence

method 2: Absolute Positional representation:  when no large dataset

use static patterns, with sine and cosine signals to encode position.

method 3: Relative Positional representation

DEBert use this representation.

Adding a classification head

The decoder

the decoder has 2 attention sublayers

- masked multi-head self-attention layer: without look future tokens, just focus past and current tokens
  - given a lower triangled masked matrix, and replace all 0s with -inf
- encoder-decoder attention layer: use curent info as query, and compute attention scores with query in decoder and key, value from encoders.
  - query: cartoon for current text (query)
  - key, value: which passage match the description(key), draw next word with cartoon(value)

Meet the transformers

encoder branches:
for nlu task: text classification, ner, question and answering
- bert: mlm + nsp
- DistilBert: low latency with knowledge distillation; 97% bert perf with 40% less mem and 60% faster
- RoBerta: improve perf, trained on large batches with more data, drops nsp.
- albert: make model efficient by allow small embeddings, all layers share same parameters, nsp replaced with sentence-ordering predition (sentence order swaped or not)

decoder branch:
predict next word; for text generation tasks.
- gpt: novel and efficient transformer decoder and transfer learning. trained on bookcorpus.
- gpt2: model larger, dataset larger, able to predict long sentence.
- ctrl: adding control tokens for style generation
- gpt3: generate more realistic texts, few-show learning (given fewer examples of new task, it can accomplish the task on new examples)

encoder-decoder branch:
- T5: unifies NLP and NLG(text2text tasks); generate text labels without classification.
- BART: input sentence undergo several transformation(sent permu, token deletion, doc rotation) and reconstruct the original texts.
